{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "- copy-paste D2V code here. Try other models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>venue</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>articledate</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmid</th>\n",
       "      <th>pii</th>\n",
       "      <th>pmc</th>\n",
       "      <th>tiabs</th>\n",
       "      <th>covid</th>\n",
       "      <th>pid</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal-short</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>issns</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BMC research notes</td>\n",
       "      <td>The prevalence and correlates of social phobia...</td>\n",
       "      <td>\\nSocial phobia is highly prevalent among univ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>10.1186/s13104-019-4482-y</td>\n",
       "      <td>31324266</td>\n",
       "      <td>10.1186/s13104-019-4482-y</td>\n",
       "      <td>PMC6642571</td>\n",
       "      <td>The prevalence and correlates of social phobia...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Journal of chemical information and modeling</td>\n",
       "      <td>Off-pocket Activity Cliffs, a Puzzling Facet o...</td>\n",
       "      <td>\\nWhile accurate quantitative prediction of li...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>10.1021/acs.jcim.9b00731</td>\n",
       "      <td>31790251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Off-pocket Activity Cliffs, a Puzzling Facet o...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Methods in molecular biology (Clifton, N.J.)</td>\n",
       "      <td>Isolation and Characterization of Plant Metabo...</td>\n",
       "      <td>\\nPseudomonas syringae is a bacterium that can...</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-1-4939-9458-8_13</td>\n",
       "      <td>31041769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Isolation and Characterization of Plant Metabo...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pharmacogenomics</td>\n",
       "      <td>Pharmacogenetic testing in primary care practi...</td>\n",
       "      <td>\\nAim: Although health authorities have set ph...</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.2217/pgs-2019-0004</td>\n",
       "      <td>31190623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pharmacogenetic testing in primary care practi...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Journal of cancer education : the official jou...</td>\n",
       "      <td>Correction to: Educational Opportunities for D...</td>\n",
       "      <td>\\nThe original version of this article unfortu...</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s13187-019-01616-0</td>\n",
       "      <td>31515717</td>\n",
       "      <td>10.1007/s13187-019-01616-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correction to: Educational Opportunities for D...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              venue  \\\n",
       "0      0                                 BMC research notes   \n",
       "1      1       Journal of chemical information and modeling   \n",
       "2      2       Methods in molecular biology (Clifton, N.J.)   \n",
       "3      3                                   Pharmacogenomics   \n",
       "4      4  Journal of cancer education : the official jou...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The prevalence and correlates of social phobia...   \n",
       "1  Off-pocket Activity Cliffs, a Puzzling Facet o...   \n",
       "2  Isolation and Characterization of Plant Metabo...   \n",
       "3  Pharmacogenetic testing in primary care practi...   \n",
       "4  Correction to: Educational Opportunities for D...   \n",
       "\n",
       "                                            abstract  year articledate  \\\n",
       "0  \\nSocial phobia is highly prevalent among univ...  2019  2019-07-19   \n",
       "1  \\nWhile accurate quantitative prediction of li...  2019  2019-12-02   \n",
       "2  \\nPseudomonas syringae is a bacterium that can...  2019         NaN   \n",
       "3  \\nAim: Although health authorities have set ph...  2019         NaN   \n",
       "4  \\nThe original version of this article unfortu...  2019         NaN   \n",
       "\n",
       "                            doi      pmid                         pii  \\\n",
       "0     10.1186/s13104-019-4482-y  31324266   10.1186/s13104-019-4482-y   \n",
       "1      10.1021/acs.jcim.9b00731  31790251                         NaN   \n",
       "2  10.1007/978-1-4939-9458-8_13  31041769                         NaN   \n",
       "3         10.2217/pgs-2019-0004  31190623                         NaN   \n",
       "4    10.1007/s13187-019-01616-0  31515717  10.1007/s13187-019-01616-0   \n",
       "\n",
       "          pmc                                              tiabs covid  pid  \\\n",
       "0  PMC6642571  The prevalence and correlates of social phobia...     0  NaN   \n",
       "1         NaN  Off-pocket Activity Cliffs, a Puzzling Facet o...     0  NaN   \n",
       "2         NaN  Isolation and Characterization of Plant Metabo...     0  NaN   \n",
       "3         NaN  Pharmacogenetic testing in primary care practi...     0  NaN   \n",
       "4         NaN  Correction to: Educational Opportunities for D...     0  NaN   \n",
       "\n",
       "  authors journal journal-short pubdate issns publisher  \n",
       "0     NaN     NaN           NaN     NaN   NaN       NaN  \n",
       "1     NaN     NaN           NaN     NaN   NaN       NaN  \n",
       "2     NaN     NaN           NaN     NaN   NaN       NaN  \n",
       "3     NaN     NaN           NaN     NaN   NaN       NaN  \n",
       "4     NaN     NaN           NaN     NaN   NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/train.csv', dtype=str)\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process\n",
    "Different algorithms and different text corpora call for different kinds of pre-processing. What we are doing here is trying to make the text as easy as possible for the computer to 'understand' without destroying any valuable features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import deaccent\n",
    "from gensim.parsing.preprocessing import strip_short, strip_punctuation, remove_stopwords, strip_multiple_whitespaces, stem_text\n",
    "from gensim.parsing import preprocess_string\n",
    "from gensim import corpora\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngramming\n",
    "We will start by building Ngrammers. These are very simple models which learn when words tend to appear together. The typical example here is 'New York', which is 2 words, but which we treat as one. Is therefore a 'bigram'. We might also be interested in 'New York City', which is 3 words that often appear together, so that's a \"trigram\".\n",
    "\n",
    "We will run 2 processes where we first search our text for bigrams and then trigrams. Essentially, it's just the same process run twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training bigram detection\n",
      "\n",
      "Training trigram detection\n",
      "trigrams trained and saved\n"
     ]
    }
   ],
   "source": [
    "bigram_phraserpath = 'models/bigram_d2v_pubmed'\n",
    "trigram_phraserpath = 'models/trigram_d2v_pubmed'\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(),\n",
    "                    deaccent,\n",
    "                    strip_punctuation,\n",
    "                    strip_multiple_whitespaces,\n",
    "                    # strip_short, #(minsize=2),\n",
    "                    ] # stem or lemmatize?\n",
    "\n",
    "# define Iterator\n",
    "\n",
    "# retrieve each text entry\n",
    "def extract_text():\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        doi = str(row['doi'])\n",
    "        textdata = str(row['tiabs'])\n",
    "        textdata = preprocess_string(textdata, CUSTOM_FILTERS)\n",
    "        yield doi, textdata\n",
    "#         if i%10000==0:\n",
    "#             print(i, 'iterations done')\n",
    "\n",
    "\n",
    "# iteration 1\n",
    "print()\n",
    "print('Training bigram detection')\n",
    "documents = extract_text()\n",
    "phrases = Phrases(\n",
    "                (document[1]\n",
    "                 for document in documents),\n",
    "                    # min_count = 10,\n",
    "                    threshold = 10,\n",
    "                    common_terms = [\"of\", \"with\", \"without\",\n",
    "                                \"and\", \"or\", \"the\", \"a\"]\n",
    "                    # max_vocab_size = 1000000\n",
    "                    )\n",
    "bigram = Phraser(phrases)\n",
    "bigram.save(bigram_phraserpath)\n",
    "\n",
    "\n",
    "# iteration 2\n",
    "print()\n",
    "print('Training trigram detection')\n",
    "documents = extract_text()\n",
    "phrases = Phrases((bigram[document[1]]\n",
    "                    for document in documents),\n",
    "                    # min_count = 10,\n",
    "                    threshold = 10,\n",
    "                    # max_vocab_size = 2000000\n",
    "                    )\n",
    "trigram = Phraser(phrases)\n",
    "# save model\n",
    "trigram.save(trigram_phraserpath)\n",
    "print('trigrams trained and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process for d2v\n",
    "from gensim.utils import deaccent\n",
    "from gensim.parsing.preprocessing import strip_short, strip_punctuation, remove_stopwords, strip_multiple_whitespaces, stem_text\n",
    "from gensim.parsing import preprocess_string\n",
    "from gensim.models import Phrases\n",
    "\n",
    "\n",
    "bigram = Phrases.load(bigram_phraserpath)\n",
    "trigram = Phrases.load(trigram_phraserpath)\n",
    "\n",
    "def pre_d2v_search(s, bigram, trigram):\n",
    "    \n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(),\n",
    "                    deaccent,\n",
    "                    strip_punctuation,\n",
    "                    strip_multiple_whitespaces] \n",
    "    \n",
    "    return trigram[bigram[preprocess_string(s,filters= CUSTOM_FILTERS)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.summarization.textcleaner import get_sentences\n",
    "from gensim.utils import tokenize\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022452, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will allow us to load our data into the doc2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ADTaggedDocument(object):\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for i,row in df.iterrows():\n",
    "            id_ = str(row['index'])\n",
    "#             doi = str(row['doi'])\n",
    "            text_data = str(row['tiabs'])\n",
    "            tokenized = pre_d2v_search(text_data, bigram, trigram)\n",
    "            yield TaggedDocument(tokenized, [id_])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we train the doc2vec model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate ~4hrs for training with 7 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# COMMENT/UNCOMMENT FOR TRAINING\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "documents = ADTaggedDocument()\n",
    "# model.build_vocab(documents)\n",
    "\n",
    "n_features = 300\n",
    "\n",
    "model = Doc2Vec(documents = documents,\n",
    "                    dm=0, \n",
    "                    dbow_words=1, \n",
    "                    vector_size=n_features, \n",
    "                    window=10, \n",
    "                    min_count=7, \n",
    "                    epochs=8, \n",
    "                    workers= cores-1\n",
    "#                     alpha=0.06,  # comment these lines to use default alpha\n",
    "#                     min_alpha=0.04\n",
    "                   )\n",
    "\n",
    "# re-initialise generator\n",
    "documents = ADTaggedDocument()\n",
    "# Now train Doc2Vec on the corpus\n",
    "model.train(documents, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)\n",
    "\n",
    "\n",
    "model.save(os.path.join('models', 'd2v_pubmed.model'), separately=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1c0f1580548>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec.load('models/d2v_pubmed.model')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform our corpus of text data using the doc2vec model to give us a 'd2v_corpus'.  We can visualise this data to get a feel for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.matutils import any2sparse\n",
    "\n",
    "\n",
    "# This should be done with batch/vector process. It's SLOW!\n",
    "\n",
    "def iter_df(df):\n",
    "    for i, row in df.iterrows():\n",
    "#         doi = str(row['doi'])\n",
    "        id_ = str(row['index'])\n",
    "#             doi = str(row['doi'])\n",
    "        textdata = str(row['tiabs'])\n",
    "        ngrams = pre_d2v_search(textdata, bigram, trigram)\n",
    "        yield id_, ngrams\n",
    "\n",
    "\n",
    "def iter_arts(df):\n",
    "    articles = iter_df(df)\n",
    "    for article in articles:\n",
    "        textdata = article[1]\n",
    "        vec = model.infer_vector(textdata) \n",
    "        vec = any2sparse(vec, eps=1e-09)\n",
    "#         print(np.shape(vec))\n",
    "        yield vec\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, similarities\n",
    "transformed = corpora.MmCorpus.serialize('data/d2v_corpus_pubmed.mm', iter_arts(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this to check the model\n",
    "index_d2v = similarities.Similarity('data/d2v_sims_pubmed.index',\n",
    "                                      corpora.MmCorpus('data/d2v_corpus_pubmed.mm'),\n",
    "                                      num_features=300)\n",
    "index_d2v.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('54', 1.0),\n",
       " ('1013896', 0.6919227838516235),\n",
       " ('415028', 0.6909884214401245),\n",
       " ('1014397', 0.6852828860282898),\n",
       " ('59599', 0.6844850778579712),\n",
       " ('544497', 0.6833879947662354),\n",
       " ('65363', 0.6744802594184875),\n",
       " ('864855', 0.6735906600952148),\n",
       " ('769358', 0.6709409356117249),\n",
       " ('71265', 0.6705315113067627)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A novel dataset on a culture of cooperation and inclusive political institutions in 90 European historical regions observed between 1000 and 1600.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_number = 84\n",
    "df.iloc[row_number]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A novel dataset on a culture of cooperation and inclusive political institutions in 90 European historical regions observed between 1000 and 1600.',\n",
       " 'Inclusive growth: A dataset on key and institutional foundations for inclusive development of Russian regions.',\n",
       " 'Feeling the Squeeze: Nonmarket Institutional Pressures and Firm Nonmarket Strategies.',\n",
       " 'Navigating through the Mundellian Trilemma: A dataset of four decades.',\n",
       " 'The interdependence of corporate reputation and ownership: a network approach to quantify reputation.',\n",
       " 'The glocalization of antimicrobial stewardship.',\n",
       " 'Macroeconomic dataset for comparative studies on coastal and inland regions in innovation space of Russia.',\n",
       " 'The credibility of scientific communication sources regarding climate change: A population-based survey experiment.',\n",
       " 'Investigating intergroup attitudes in Europe: Cross-national data on news media, attitudes towards newcomers, and socio-psychological indicators.',\n",
       " 'Measuring sustainable development goals performance: How to monitor policy action in the 2030 Agenda implementation?']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vec = model.docvecs[row_number]\n",
    "# similar articles\n",
    "sims = model.docvecs.most_similar([query_vec])\n",
    "sims_inds = [x[0] for x in sims] #np.argsort(sims)[::-1]\n",
    "cols = ['doi','title']\n",
    "df[cols].iloc[sims_inds]['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpora.MmCorpus.serialize('data/d2v_corpus_pubmed.mm', iter_arts(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorise test and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv', dtype = str).reset_index()\n",
    "corpora.MmCorpus.serialize('data/d2v_test_corpus_pubmed.mm', iter_arts(test))\n",
    "dev = pd.read_csv('data/dev.csv', dtype = str).reset_index()\n",
    "corpora.MmCorpus.serialize('data/d2v_dev_corpus_pubmed.mm', iter_arts(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = pd.read_csv('data/all_s2_pubmed.csv', dtype = str).reset_index()\n",
    "corpora.MmCorpus.serialize('data/d2v_all_corpus_pubmed.mm', iter_arts(all_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional reduction for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.mmcorpus.MmCorpus at 0x1bfd03b0ee8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_dim = corpora.MmCorpus('data/d2v_all_corpus_pubmed.mm')\n",
    "high_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032546, 300)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "X = corpus2dense(high_dim, num_terms = n_features).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-uMap\n",
    "This is a fast way to shrink our dataset to make it more manageable for Umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032546, 50)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# pick an intermediate dimensionality to start off dimensional reduction, then let uMap do the rest\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "X_reduced = TruncatedSVD(n_components = 50,\n",
    "                            random_state = 0).fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "trans = umap.UMAP(\n",
    "#                 n_neighbors=5,\n",
    "#                   min_dist=0.1,\n",
    "#                   metric='correlation'\n",
    "                 ).fit(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032546, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/2d_s2_pubmed_d2v_corpus.npy',trans.embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
